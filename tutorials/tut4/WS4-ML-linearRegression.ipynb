{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Workshop: Week 4Â¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge and LASSO regressions\n",
    "We now want to understand Ridge and LASSO regression models. \n",
    "There are two reasons that we are not satisfied with the Least Squares Estimators (LSE):\n",
    "a) The prediction accuracy: the LSE often have low bias but large variance, which indicates that the prediction often has low accuracy. The prediction accuracy often could be improved by shrinking or setting some coefficients to zero.\n",
    "b) The interpretation: LSE provides estimates of coefficients. When there are large number of predictors, we often would like to detimine a smaller subset that exhibit the strongest effects. In order to get a big picture, we are willing to sacrifice some of the small details.\n",
    "\n",
    "To understand Ridge and LASSO regressions, I strongly recomment to read the Chapter 3 in the book: T. Hastie, R. Tibsirani, & J. Friedman, The Elements of Statistical Learning, Springer, Second Edition, 2009.\n",
    "\n",
    "Ridge Regression:\n",
    "The ridge regression shrinks the regression coefficients by adding a quadratic penalty, which is to solve the following optimization problem:\n",
    "$$\\hat w^{ridge} = argmin_w \\sum_{i=1}^N(y_i - w_0 - \\sum_{j = 1}^d w_j x_{ij})^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n",
    "where $\\lambda\\geq0$ is a penalty controling the amount of shrinkage: the larger the value of $\\lambda$, the greater the amount of shrinkage.   \n",
    "\n",
    "LASSO regression:\n",
    "Similar to redge regression, the LASSO regression adds a penalty to the coefficients which imposes a greater shrinkage. It is to solve the following problem:\n",
    "$$\\hat w^{lasso} = argmin_w \\sum_{i=1}^N(y_i - w_0 - \\sum_{j = 1}^d w_j x_{ij})^2 + \\lambda \\sum_{j = 1}^d |w_j|$$\n",
    "\n",
    "To better understand these algorithms, we work on the following data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prostate Cancer Example\n",
    "In this example, we want to explore the correlation between the level of prostate-specific antigen (lpsa) and a number of clinical measures in men who were about to receive a radical prostatectomy. These variables are log cancer volume (lcavol), log prostate weight (lweight), age, log of the amount of benigh prostatic hyperplasia (lbph), seminal vesicle invasion (svi), log of capsular penetration (lcp), Gleason score (gleason), and percent of Gleason scores 4 or 5 (pgg45).\n",
    "\n",
    "You can load the data prostate_dataset.txt provided. In this excercise, you are asked to apply the LSE, Ridge and LASS regressions to the data. You can use Python modules such as scikit-learn to implement these algorithms. Our target is to understand the differences between these algorithms. After you have applied these algorithms, can you identify which are the factors affecting lpsa? You can try different experiments, for example, varying the parameter $\\lambda$ for ridge and lasso model.\n",
    "\n",
    "A further question: can you choose the best model from LSE, Ridge and LASSO for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    col    lcavol   lweight  age      lbph  svi       lcp  gleason  pgg45  \\\n",
      "0     1 -0.579818  2.769459   50 -1.386294    0 -1.386294        6      0   \n",
      "1     2 -0.994252  3.319626   58 -1.386294    0 -1.386294        6      0   \n",
      "2     3 -0.510826  2.691243   74 -1.386294    0 -1.386294        7     20   \n",
      "3     4 -1.203973  3.282789   58 -1.386294    0 -1.386294        6      0   \n",
      "4     5  0.751416  3.432373   62 -1.386294    0 -1.386294        6      0   \n",
      "5     6 -1.049822  3.228826   50 -1.386294    0 -1.386294        6      0   \n",
      "6     7  0.737164  3.473518   64  0.615186    0 -1.386294        6      0   \n",
      "7     8  0.693147  3.539509   58  1.536867    0 -1.386294        6      0   \n",
      "8     9 -0.776529  3.539509   47 -1.386294    0 -1.386294        6      0   \n",
      "9    10  0.223144  3.244544   63 -1.386294    0 -1.386294        6      0   \n",
      "10   11  0.254642  3.604138   65 -1.386294    0 -1.386294        6      0   \n",
      "11   12 -1.347074  3.598681   63  1.266948    0 -1.386294        6      0   \n",
      "12   13  1.613430  3.022861   63 -1.386294    0 -0.597837        7     30   \n",
      "13   14  1.477049  2.998229   67 -1.386294    0 -1.386294        7      5   \n",
      "14   15  1.205971  3.442019   57 -1.386294    0 -0.430783        7      5   \n",
      "15   16  1.541159  3.061052   66 -1.386294    0 -1.386294        6      0   \n",
      "16   17 -0.415515  3.516013   70  1.244155    0 -0.597837        7     30   \n",
      "17   18  2.288486  3.649359   66 -1.386294    0  0.371564        6      0   \n",
      "18   19 -0.562119  3.267666   41 -1.386294    0 -1.386294        6      0   \n",
      "19   20  0.182322  3.825375   70  1.658228    0 -1.386294        6      0   \n",
      "20   21  1.147402  3.419365   59 -1.386294    0 -1.386294        6      0   \n",
      "21   22  2.059239  3.501043   60  1.474763    0  1.348073        7     20   \n",
      "22   23 -0.544727  3.375880   59 -0.798508    0 -1.386294        6      0   \n",
      "23   24  1.781709  3.451574   63  0.438255    0  1.178655        7     60   \n",
      "24   25  0.385262  3.667400   69  1.599388    0 -1.386294        6      0   \n",
      "25   26  1.446919  3.124565   68  0.300105    0 -1.386294        6      0   \n",
      "26   27  0.512824  3.719651   65 -1.386294    0 -0.798508        7     70   \n",
      "27   28 -0.400478  3.865979   67  1.816452    0 -1.386294        7     20   \n",
      "28   29  1.040277  3.128951   67  0.223144    0  0.048790        7     80   \n",
      "29   30  2.409644  3.375880   65 -1.386294    0  1.619388        6      0   \n",
      "..  ...       ...       ...  ...       ...  ...       ...      ...    ...   \n",
      "67   68  2.198335  4.050915   72  2.307573    0 -0.430783        7     10   \n",
      "68   69 -0.446287  4.408547   69 -1.386294    0 -1.386294        6      0   \n",
      "69   70  1.193922  4.780383   72  2.326302    0 -0.798508        7      5   \n",
      "70   71  1.864080  3.593194   60 -1.386294    1  1.321756        7     60   \n",
      "71   72  1.160021  3.341093   77  1.749200    0 -1.386294        7     25   \n",
      "72   73  1.214913  3.825375   69 -1.386294    1  0.223144        7     20   \n",
      "73   74  1.838961  3.236716   60  0.438255    1  1.178655        9     90   \n",
      "74   75  2.999226  3.849083   69 -1.386294    1  1.909542        7     20   \n",
      "75   76  3.141130  3.263849   68 -0.051293    1  2.420368        7     50   \n",
      "76   77  2.010895  4.433789   72  2.122262    0  0.500775        7     60   \n",
      "77   78  2.537657  4.354784   78  2.326302    0 -1.386294        7     10   \n",
      "78   79  2.648300  3.582129   69 -1.386294    1  2.583998        7     70   \n",
      "79   80  2.779440  3.823192   63 -1.386294    0  0.371564        7     50   \n",
      "80   81  1.467874  3.070376   66  0.559616    0  0.223144        7     40   \n",
      "81   82  2.513656  3.473518   57  0.438255    0  2.327278        7     60   \n",
      "82   83  2.613007  3.888754   77 -0.527633    1  0.559616        7     30   \n",
      "83   84  2.677591  3.838376   65  1.115142    0  1.749200        9     70   \n",
      "84   85  1.562346  3.709907   60  1.695616    0  0.810930        7     30   \n",
      "85   86  3.302849  3.518980   64 -1.386294    1  2.327278        7     60   \n",
      "86   87  2.024193  3.731699   58  1.638997    0 -1.386294        6      0   \n",
      "87   88  1.731656  3.369018   62 -1.386294    1  0.300105        7     30   \n",
      "88   89  2.807594  4.718052   65 -1.386294    1  2.463853        7     60   \n",
      "89   90  1.562346  3.695110   76  0.936093    1  0.810930        7     75   \n",
      "90   91  3.246491  4.101817   68 -1.386294    0 -1.386294        6      0   \n",
      "91   92  2.532903  3.677566   61  1.348073    1 -1.386294        7     15   \n",
      "92   93  2.830268  3.876396   68 -1.386294    1  1.321756        7     60   \n",
      "93   94  3.821004  3.896909   44 -1.386294    1  2.169054        7     40   \n",
      "94   95  2.907447  3.396185   52 -1.386294    1  2.463853        7     10   \n",
      "95   96  2.882564  3.773910   68  1.558145    1  1.558145        7     80   \n",
      "96   97  3.471966  3.974998   68  0.438255    1  2.904165        7     20   \n",
      "\n",
      "        lpsa train  \n",
      "0  -0.430783     T  \n",
      "1  -0.162519     T  \n",
      "2  -0.162519     T  \n",
      "3  -0.162519     T  \n",
      "4   0.371564     T  \n",
      "5   0.765468     T  \n",
      "6   0.765468     F  \n",
      "7   0.854415     T  \n",
      "8   1.047319     F  \n",
      "9   1.047319     F  \n",
      "10  1.266948     T  \n",
      "11  1.266948     T  \n",
      "12  1.266948     T  \n",
      "13  1.348073     T  \n",
      "14  1.398717     F  \n",
      "15  1.446919     T  \n",
      "16  1.470176     T  \n",
      "17  1.492904     T  \n",
      "18  1.558145     T  \n",
      "19  1.599388     T  \n",
      "20  1.638997     T  \n",
      "21  1.658228     F  \n",
      "22  1.695616     T  \n",
      "23  1.713798     T  \n",
      "24  1.731656     F  \n",
      "25  1.766442     F  \n",
      "26  1.800058     T  \n",
      "27  1.816452     F  \n",
      "28  1.848455     T  \n",
      "29  1.894617     T  \n",
      "..       ...   ...  \n",
      "67  2.962692     T  \n",
      "68  2.962692     T  \n",
      "69  2.972975     T  \n",
      "70  3.013081     T  \n",
      "71  3.037354     T  \n",
      "72  3.056357     F  \n",
      "73  3.075006     F  \n",
      "74  3.275256     T  \n",
      "75  3.337547     T  \n",
      "76  3.392829     T  \n",
      "77  3.435599     T  \n",
      "78  3.457893     T  \n",
      "79  3.513037     F  \n",
      "80  3.516013     T  \n",
      "81  3.530763     T  \n",
      "82  3.565298     T  \n",
      "83  3.570940     F  \n",
      "84  3.587677     T  \n",
      "85  3.630986     T  \n",
      "86  3.680091     T  \n",
      "87  3.712352     T  \n",
      "88  3.984344     T  \n",
      "89  3.993603     T  \n",
      "90  4.029806     T  \n",
      "91  4.129551     T  \n",
      "92  4.385147     T  \n",
      "93  4.684443     T  \n",
      "94  5.143124     F  \n",
      "95  5.477509     T  \n",
      "96  5.582932     F  \n",
      "\n",
      "[97 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('prostate_dataset.txt',delimiter=\"\\t\")\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
